{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dd24fe",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction using Naive Bayes and K-Nearest Neighbours\n",
    "Implemented from scratch using Numpy, Pandas, and Matplotlib.\n",
    "\n",
    "**Dataset:** Titanic - Machine Learning from Disaster\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Data preprocessing and visualization\n",
    "- Naive Bayes and K-Nearest Neighbours from scratch\n",
    "- Evaluation of predictions using accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "gender_submission = pd.read_csv(\"gender_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff034ce",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Drop unused columns: `Cabin`, `Ticket`, `Name`, `PassengerId`\n",
    "- Fill missing `Age` with median\n",
    "- Encode `Sex` and `Embarked`\n",
    "- Normalize `Age` and `Fare` for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26919b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df = df.drop(columns=[\"Cabin\", \"Ticket\", \"Name\", \"PassengerId\"], errors='ignore')\n",
    "    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n",
    "    df[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\n",
    "    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
    "\n",
    "    le_sex = LabelEncoder()\n",
    "    le_embarked = LabelEncoder()\n",
    "\n",
    "    df[\"Sex\"] = le_sex.fit_transform(df[\"Sex\"])\n",
    "    df[\"Embarked\"] = le_embarked.fit_transform(df[\"Embarked\"])\n",
    "\n",
    "    df[\"Age\"] = (df[\"Age\"] - df[\"Age\"].mean()) / df[\"Age\"].std()\n",
    "    df[\"Fare\"] = (df[\"Fare\"] - df[\"Fare\"].mean()) / df[\"Fare\"].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "train_processed = preprocess(train)\n",
    "X_train = train_processed.drop(\"Survived\", axis=1).values\n",
    "y_train = train_processed[\"Survived\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ad57a",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier (from scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_train(X, y):\n",
    "    classes = np.unique(y)\n",
    "    summaries = {}\n",
    "\n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        summaries[c] = {\n",
    "            \"mean\": X_c.mean(axis=0),\n",
    "            \"var\": X_c.var(axis=0) + 1e-9,  # Avoid division by zero\n",
    "            \"prior\": X_c.shape[0] / X.shape[0]\n",
    "        }\n",
    "    return summaries\n",
    "\n",
    "def gaussian_prob(x, mean, var):\n",
    "    exponent = np.exp(- ((x - mean) ** 2) / (2 * var))\n",
    "    return (1 / np.sqrt(2 * np.pi * var)) * exponent\n",
    "\n",
    "def naive_bayes_predict(X, summaries):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        posteriors = {}\n",
    "        for c, params in summaries.items():\n",
    "            prior = np.log(params[\"prior\"])\n",
    "            likelihood = np.sum(np.log(gaussian_prob(x, params[\"mean\"], params[\"var\"])))\n",
    "            posteriors[c] = prior + likelihood\n",
    "        y_pred.append(max(posteriors, key=posteriors.get))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "nb_model = naive_bayes_train(X_train, y_train)\n",
    "nb_predictions = naive_bayes_predict(X_train, nb_model)\n",
    "nb_accuracy = (nb_predictions == y_train).mean()\n",
    "print(f\"Naive Bayes Training Accuracy: {nb_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3bf12",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours (from scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff8082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k=5):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in X_train]\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = y_train[k_indices]\n",
    "        values, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "        predictions.append(values[np.argmax(counts)])\n",
    "    return np.array(predictions)\n",
    "\n",
    "knn_preds = knn_predict(X_train, y_train, X_train, k=5)\n",
    "knn_accuracy = (knn_preds == y_train).mean()\n",
    "print(f\"KNN Training Accuracy (k=5): {knn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8723a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Naive Bayes performed well given the assumptions of feature independence.\n",
    "- KNN was sensitive to feature scaling and choice of `k`.\n",
    "\n",
    "Both classifiers were implemented without using any ML libraries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
